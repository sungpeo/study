Deploy Mode) Cluster 모드에서
1. Spark가 Cluster Manager로부터 어떻게 리소스를 얻어오고,
2. 어떻게 Executor를 할당하는지

-------------------------------------------------

# SparkSubmit.scala

##

```scala
submit = new SparkSubmit()
submit.doSubmit(args)
```

## 80: doSubmit
 - intializeLogIfNecessary // log4j
 - parseArguments(args) // valdation ex) yarn은 HADOOP_CONF_DIR or YARN_CONF_DIR 환경 변수가 지정되어 있어야함.
 - appArgs.action match { ...

## 156: @tailrec submit

```scala
proxyUser.doAs(...
  runMain
```

## 893: runMain

## 220: prepareSubmitEnvironment

## 231: 여기서 일단 args.master 를 구분함

```scala
val clusterManager = ...
  yarn, spark, mesos, k8s, local
```

## 243: deployMode
## 306

```scala
// Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files
// too for packages that include Python code

val resolvedMavenCoordinates = DependencyUtils.resolveMavenDependencies(...
```

dependency 관련 설정, maven resolve에 ivy 내용을 포함한다.
그 결과를 어떻게 처리하는 지는 환경(cluster_manager, deploy_mode)에 따라 다르다.
`resolvedMavenCoordinates`는 args.jars와 args.pyFiles에 추가한다.

## 340: update spark config from args
여기서는 SparkHadoopUtils.newConfiguration을 통해 설정을 읽어들이는데, (S3, SparkHadoop, SparkHive 관련한 기본 설정을 포함한다.)

## 345: kerberos 지원 (standalone, mesos는 미지원)
deployMode가 client인 경우 args.keytab path를 LocalURIi로 저장

args.keytab이 localUri가 아닌 경우, 파일 존재 여부를 확인한 뒨, keytab으로 로그인한다.

```scala
    if (!Utils.isLocalUri(args.keytab)) {
        require(new File(args.keytab).exists(), s"Keytab file: ${args.keytab} does not exist")
        UserGroupInformation.loginUserFromKeytab(args.principal, args.keytab)
    }
```
args로 입력 받은 jars, files, pyFiles, archives 는 local, http, https, ftp schema 외에는 fs(hadoopConf)로 globStatus한 결과를 하나씩 풀어서 File인 것만 추려낸다.

## 432: clusterManager == YARN 인 경우

hadoop filesystem이 지원하지 않는 remote resource에 대해
forceDownloadSchems 설정에 따라 해당 scheme 파일의 경우 targetDir(created temp dir)로 download할 수 있도록 메서드를 정의한다.

## 455: downloadResource
primaryResource, files, pyFiles, jars, archives

## 472: args.mainClass가 지정되지 않은 경우 jar에 한해 primaryResource를 JarInput으로 읽어들여 Main-Class가 정의되었는지 확인한다. 없다면 error

## 521: isR && YARN
sparkR 관련 패키지 파일(SparkR 및 R packages)들을 args.arhives에 포함시킨다.
(mesos에선 지원 안함)

## 573: isYarnCluster && args.isR
primaryResource 들도 클러스터에 배포될 수 있도록 arg.files에 포함한다.

## 584: OptionAssigner 를 통해 각 배포 환경에 맞는 설정을 지정한다.

```scala
options - List[OptionAssigner]( ...
```

## 671: isYarnCluster
(659: yarn client 모드의 경우 localPrimaryResource, localJars)

childClasspath에 args.primaryResource 와 args.jars를 추가한다. 

## 735: YARN, args.isPython은 spark.yarn.isPython true로 설정

## 747: yarn cluster mode의 경우 yarn.Client를 mainClass로 wrapping해서 작업 제출해야하므로 관련 설정을 해준다. 
org.apache.spark.deploy.yarn.YarnClusterApplication

(참고: 767: Mesos cluster mode는 API로 지원)
(참고: 787: KubernetesCluster의 경우 yarn과 비슷한 형태로 KubernetesClientApplication 으로 제출) 

## 821: cluster mode인 경우 DRIVER_HOST_ADDRESS 삭제

## 856: 여기까지가 전부 prepareSubmitEnvironment

```scala
(childArgs.toSeq, childClasspath.toSeq, sparkConf, childMainClass)
```

## 894: runMain으로 돌아와서..

## 934: yarn-cluster 경우, mainClass가 YarnClusterApp이므로 JavaMainApplication으로 app: SparkApplication에 할당된다.


## 951: start
app.start(childArgs.toArray, sparkConf)

JavaMainApplication의 start 구현을 보면, (SparkApplication.scala)
sparkConf는 system properties로 설정해주고,
java reflection으로 static main method를 실행하면서 childArgs 를 넘겨준다.

--------------------------------------------

# yarn/Client.scala

## 1625: YarnClusterApplication extends SparkApplication

## 1627: start

yarn에서는 실행에 필요한 파일들을 yarn cache 형태로 배포해서 사용하기 때문에 spark 기능을 위한 설정을 삭제한다.
- conf.remove JARS, FILES, and ARCHIVES

new Client(new ClientArguments(args), conf, null).run()


## 65: class Client(...

## 45: org.apache.hadoop 에 대한 dependencies가 본격적으로 있음..

yarn/Client 코드는 직접적으로  하둡 영역과 맞닿아 있음.
참고: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html

## 74: yarnClient = YarnClient.createYarnClient
참고: import org.apache.hadoop.yarn.client.api.{YarnClient, YarnClientApplication}

이런 식으로 class가 정의되어 있고 우린 여기서 동작 흐름에 따라 run() 부터 살펴보자.

## 1226: run()
```scala
def run(): Unit = { ...
```

## 1227: submitApplication and set appId
```scala
this.appId = submitApplication()
```

## 169: def submitApplication(): ApplicationId = { ...
```scala
launcherBackend.connect()
yarnClient.init(hadoopConf)
yarnClient.start()

// ...

// Get a new application from our RM
val newApp = yarnClient.createApplication()
val newAppResponse = newApp.getNewApplicationResponse()
appId = newAppResponse.getApplicationId()

// ...

// Verify whether the cluster has enough resources for our AM
verifyClusterResources(newAppResponse)

// Set up the appropriate contexts to launch our AM
val containerContext = createContainerLaunchContext(newAppResponse)
val appContext = createApplicationSubmissionContext(newApp, containerContext)

// Finally, submit and monitor the application
logInfo(s"Submitting application $appId to ResourceManager")
yarnClient.submitApplication(appContext)
launcherBackend.setAppId(appId.toString)
reportLauncherState(SparkAppHandle.State.SUBMITTED)
```

이런 식으로 submit이 이뤄짐
복잡한 설정이 이뤄진 createContinaerLaunchContext 와 createApplicationSubmissionContext만 조금 더 자세히 보자.

## 875 createContainerLaunchContext

## 886 set amContainer
```scala
// populateClasspath: env classpath에 필요한 dependencies 지정
// python 설정
val launchEnv = setupLaunchEnv(stagingDirPath, pySparkArchives)

// upload any reousrces to the distributed cache if needed.
val localResources = prepareLocalResources(stagingDirPath, pySparkArchives)

val amContainer = Records.newRecord(classOf[ContainerLaunchContext])
amContainer.setLocalResources(localResources.asJava)
amContainer.setEnvironment(launchEnv.asJava)
```

### 926 isClusterMode로 달라지는 부분
clusterMode
 - DRIVER_JAVA_OPTIONS를 javaOpts로 사용
 - spark.driver.libraryPaths를 prefixEnv로 지정 (driver를 AM에 포함해서 구동시켜야 하므로)
 - AM_JAVA_OPTIONS은 cluster mode에서 효과 없음
 - userClass,  userJar, primaryPyFile, primaryRFile 지정
 - amClass 가 다름
   - cluster: org.apache.spark.deploy.yarn.ApplicationMaster
   - client: org.apache.spark.deploy.yarn.ExecutorLauncher
 
## 248: createApplicatioinSubmissionContext
위에서와 마찬가지로 spark.yarn.drive.resource가 cluster 관련 설정이고,
spark.yarn.am.resource가 client 관련 설정임을 알수 있다.

isClientUnmanagedAMEnable: unmanaged am으로 spark client를 띄울 수 있다. (default: false)


--------------------------------------------
# ApplicationMaster.scala

cluster mode인 경우,
amClass로 이걸 실행시켰으니 더 자세히 보기 위해선 이제 이 클래스를 따라가야할 차례다.


--------------------------------------------

# SparkPi

```scala
// scalastyle:off println
package org.apache.spark.examples
import org.apache.spark.sql.SparkSession

/** Computes an approximation to pi */
object SparkPi {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("Spark Pi")
      .getOrCreate()
    val slices = if (args.length > 0) args(0).toInt else 2
    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
    val count = spark.sparkContext.parallelize(1 until n, slices).map { i =>
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (x*x + y*y <= 1) 1 else 0
    }.reduce(_ + _)
    println(s"Pi is roughly ${4.0 * count / (n - 1)}")
    spark.stop()
  }
}
```

--------------------------------------------

# SparkSession.scala


## 780: object SparkSession extends Logging {

## 786: class Builder extends Logging {

## 911: def getOrCreate(): SparkSession = synchronized {

## 915: EXECUTOR_ALLOW_SPARK_CONTEXT (SparkContext can be created in executors. default: false)

## 916: assertOnDriver(): TaskContext.get 값이 있으면(!= null) IllegalStateException 발생

running task 정보를 가져올 수 없다는 걸 토대로 executor가 아님, 즉 driver임을 확인하는듯?

## 920: activeThreadSession
threadLocal에 session을 저장해두고 get해서 쓸수 있게 하고 있음. staic conf와 others로 나눠서 others의 경우 새로 설정해 줌.
(그런데도 불안한지? 혹은 문제가 있어서 logWarning이 있음)
 - static 한 설정은 변화가 없을 것이다.
 - others에 있는 core 설정은 효과가 없을 수 있다.
   - 보니까 이건 context 관련한 설정의 경우 존재하는 context가 있다면 그걸 변경하지 않고 재사용하기 때문인 것으로 보임.
   (SparkContext.scala:2679)

## 927: SparkSession.synchronized {
defaultSession

## 936: defaultSession 도 null 이거나 session.sparkContext.isStopped 라면  SparkSession을 생성

먼저 spark context가 있어야 하는데,
 - userSuppliedContext에서 가져오거나 (builder 에서 sparkContext로 지정해서 특정 context를 지정할 수 있음)
 - SparkContext.getOrCreate(sparkConf)

 // 어차피 SparkContext는 JVM에 하나만 존재함
```scala
session = new SparkSession(sparkContext, None, None, extensions, options.toMap)
setDefaultSession(session)
setActiveSession(session)
// SparkApplication 종료 시점에 defaultSession에 null 설정해주기 위함
registerContextListener(sparkContext)
```

## class SparkSession
(SparkSession.scala:81 ~ 776)

---------------------------------

그러면 SparkContext는 어떻게 만들어지고 사용되는가?

# SparkContext.scala

## 82 ~

## 87: EXECUTOR_ALLOW_SPARK_CONTEXT 값에 따라 spark session과 같은 방법으로 assertOnDriver


## 557: 만들어지는 내용중에 보다보니 task scheduler를 생성하는 부분이 있다.
DagScheduler 보다 하위 개념인(?) TaskScheduler의 구조를 확인하기로

TaskScheduler를 통해 작업을 계획할 테니 이 지점에서 clusterManager와의 접점이 있을 거라 판단.

## 2955: val cm = getClusterManager(masterUrl)
## 2960: TaskScheduler

```scala
val scheduler = cm.createTaskScheduler(sc, masterUrl)
val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
cm.initialize(scheduler, backend)
```

----------------------------------
# TaskSchedulerImpl.scala
 - 말 그대로 executor에게 task들을 스케줄링하는 역할 (backend를 주입 받도록 (initialize 시점에)

# YarnScheduler.scala < TaskSchedulerImpl
 - SparkRackResolver를 통해 hadoopConfiguration에서 랙 정보를 사용 가능하도록 함. (getRacksForHosts)

# YarnClusterScheduler < YarnScheduler
 - postStartHook에 `ApplicationMaster.sparkContextInitialized(sc)` 내용만 추가

# YarnClusterManager.scala
# YarnClusterSchedulerBackend
----------------------------------

결국은 executor 들에게 
