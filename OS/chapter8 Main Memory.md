# Chapter8. Main Memory

The memory-management algorithms vary from a primitive bare-machine approach to paging and segmentation strategies. Selection of a memory-management method for a specific system depends on many factors, especially on the hardware design of the system.

## Chapter Objectives

메모리 관리 알고리즘은 primitive bare-machine 접근방법에서부터 paging and segmentation 전략까지 다양하다. 시스템을 위한 메모리 관리 선택은 여러 factor의 영향을 받는데, 특별히 하드웨어 시스템 설계에 따른 영향을 받는다.

* To provide a detailed description of various ways of organizing memory hardware
* To explore various techniques of allocating memory to processes.
* To discuss in detail how paging works in contemporary computer systems.

## 8.1 Background

chapter1에서 살펴본 것처럼 메모리는 현대 컴퓨터 시스템의 동작에 주요한 역할을 한다. 메모리는 큰 바이트들의 배열을 가지며, 각각에 대한 주소값을 가지고 있다. CPU는 program counter의 값에 따라 메모리에서 명령(instructions)을 가져온다. 이 명령어들은 특정 메모리 주소에 대한 추가적인 loading과 storing을 발생시킬 수 있다.
예를 들어, 전형적인 instruction-execution 사이클이 메모리로부터 처음 an instruction을 가져왔다. 그 instruction은 디코딩된 후에 또 다른 operands를 메모리로부터 가져오도록 만들 수 있다. 그 instruction이 그 operands까지 수행된 후에 결과는 memory에 다시 저장될 것이다. The memory unit sees only a stream of memory addresses; it does not know how they are generated (by the instruction counter, indexing, indirection,literal addresses, and so on) or what they are for (instructions or data). 따라서 우리는 program이 어떻게 메모리 주소를 만들어내는지는 무시해도 된다. 우리는 오직 수행 중인 프로그램이 메모리 주소를 어떤 순서로 만들어내는 지 관심 있을 뿐이다.
우리는 메모리 관리와 관련된 몇 가지 문제를 다뤄보려고 한다: basic hardware, the binding of symbolic memory addresses to actual physical addresses, and the distinction between logical and physical addresses. 그리고 dynamic linking과 shared libararies에 대해 알아보며 섹션을 마무리할 것이다.

### 8.1.1 Basic Hardware

프로세서에 설치된(built into) 메인 메모리와 레지스터는 CPU가 직접 접근하는 일반 목적의 스토리지일 뿐이다. arguments로써 메모리 주소를 가져오는 기계 명령(machine instructions)은 있으나, disk 주소를 가져오는 것은 없다. 그러므로 수행 중인 명령이든 data건, 반드시 이런 direct-access storage devices들 중에 하나 안에 존재해야 한다.
CPU 내부에 있는 레지스터는 일반적으로 하나의 CPU clock cycle 내에서 접근 가능하다. 대부분의 CPU들은 instruction을 decode할 수 있고, clock tick마다 하나 이상의 operation을 레지스터 위(on register contents)에서 수행할 수 있다. The same cannot be said of main memory, which is accessed via a transaction on the memory bus. memory access를 완료하는 것은 많은 cycle of the CPU clock이 걸릴 수 있다. 이런 경우에 프로세서는 stall (멈춤)이 필요하다. 왜냐하면 프로세서가 수행중인 instruction을 완료시키위에 필요한 data가 없기 때문이다. 이 상황은 메모리 접근의 frequency 때문에 intolerable하다. 해결법은 CPU와 main memory 사이에 fast memory를 추가하는 것이다. 전형적으로 fast access를 위한 CPU chip 위에 위치한다. (cache) CPU에 내장 된 캐시를 관리하기 위해 하드웨어는 운영 체제 제어없이 메모리 액세스 속도를 자동으로 높인다.

각각의 프로세스는  독립된 메모리 공간을 가지며 특정 프로세스만 접근할 수 있는 합법적인(Legal) 메모리 주소 영역을 설정하고, 프로세스가 합법적인 영역만을 접근하도록 하는 것이 필요하다. 이를 기준(Base)과 상한(Limit)이라고 불리는 두 개의 레지스터들을 사용하여 보호 기법을 제공한다.
레지스터로 user mode 에서의 생성된 주소값을 모두 비교하여, 다른 사용자의 memory나 OS memory로의 접근을 fatal error로 처리하여 막는다. 이 구성표(scheme)는 사용자 프로그램이 (실수로 또는 의도적으로) 운영 체제 또는 다른 사용자의 코드 또는 데이터 구조를 수정하는 것을 방지한다.

### 8.1.2 Adress Binding

Usually, a program resides on a disk as a binary executable file. To be executed, the program must be brought into memory and placed within a process. Depending on the memory management in use, the process may be moved between disk and memory during its execution. The processes on the disk that are waiting to be brought into memory for execution form the **input queue**.

In most cases, a user program goes through several steps—some of which may be optional—before being executed (Figure 8.3). Addresses may be represented in different ways during these steps. Addresses in the source program are generally symbolic (such as the variable count). A compiler typically binds these symbolic addresses to relocatable addresses (such as “14 bytes from the beginning of this module”). The linkage editor or loader in turn binds the relocatable addresses to absolute addresses (such as 74014). Each binding is a mapping from one address space to another.

* **Compile time.** 컴파일 타임에 프로세스가 위치할 메모리 주소를 알고 있다면, absolute code가 생성될 것이다. 나중에 starting location이 변하면, 이 코드는 recompile이 필요하다.

* **Load time.** 컴파일 타임에 모른다면 컴파일러는 relocatable code를 생성할 것이다. 이 경우 binding은 load time까지 미뤄진다.

* **Execution time.** 프로세스가 실행 중에 옮겨질 수 있다면, binding은 런타임까지 연기되어야만 한다. 특별한 하드웨어가 이 작업을 위해 필요한데, 8.1.3에서 다룰 것이다. Most general-purpose operating systems use this method.

### 8.1.3 Logical versus Physical Address Space

An address generated by the CPU is commonly referred to as a logical address, whereas an address seen by the memory unit—that is, the one loaded into the memory-address register of the memory—is commonly referred to as a physical address.

#### 논리 주소와 물리 주소

* Logical address – generated by the CPU; also referred to as virtual address
* Physical address – address seen by the memory unit

#### Memory-Management Unit (MMU)

* CPU가 메모리에 접근하는 것을 관리하는 컴퓨터 하드웨어 부품
* 가상 메모리 주소를 실제 메모리 주소로 변환

#### Relocation Register

* In MMU scheme, the value in the relocation register is added to every address generated by a user process at the time it is sent to memory

The user program generates only logical addresses and thinks that the process runs in locations 0 to max. However, these logical addresses must be mapped to physical addresses before they are used.

### 8.1.4 Dynamic Loading

To obtain better memory-space utilization, we can use **dynamic loading**. With dynamic loading, a routine is not loaded until it is called.

The advantage of dynamic loading is that a routine is loaded only when it is needed. This method is particularly useful when large amounts of code are needed to handle infrequently occurring cases, such as error routines. In this case, although the total program size may be large, the portion that is used may be much smaller.

Dynamic loading does not require special support from the operating system. It is the responsibility of the users to design their programs to take advantage of such a method. Operating systems may help the programmer, however, by providing library routines to implement dynamic loading.

### 8.1.5 Dynamic Linking and Shared Libraries

**Dynamically linked libraries** are system libraries that are linked to user programs when the programs are run (refer back to Figure 8.3). Some operating systems support only **static linking**, in which system libraries are treated like any other object module and are combined by the loader into the binary program image.

Here, though, linking, rather than loading, is postponed until execution time. This feature is usually used with system libraries, such as language subroutine libraries.

With dynamic linking, a **stub** is included in the image for each library routine reference. The stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present.

Without dynamic linking, all such programs would need to be relinked to gain access to the new library. So that programs will not accidentally execute new, incompatible versions of libraries, version information is included in both the program and the library. Thus, only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it. Other programs linked before the new library was installed will continue using the older library. This system is also known as **shared libraries**.

Unlike dynamic loading, dynamic linking and shared libraries generally require help from the operating system. If the processes in memory are protected from one another, then the operating system is the only entity that can check to see whether the needed routine is in another process’s memory space or that can allow multiple processes to access the same memory addresses. We elaborate on this concept when we discuss paging in Section 8.5.4.

## 8.2 Swapping

process가 수행되려면 반드시 메모리 내에 있어야한다. 그러나 임시로 backing store로 swapp 될 수 있고, 다시 수행되기 위해 memory에 다시 올려서 사용된다. total physical address space보다 큰 크기를 사용해서 multiprogramming 정도를 높일 수 있다.

### 8.2.1 Standard Swapping

표준 swapping은 메인 메모리와 backing store 사이에서 프로세스를 이동시킴으로 동작한다. backing store는 일반적으로 fast disk가 사용된다. all memory images의 복사본을 담을 정도로 크기도 충분히 커야하며, 저장된 memory images로의 직접 접근을 제공해야한다. 시스템을 **ready queue**를 통해 backing store나 in memory에 있는 프로세스면서 ready to run인 모든 프로세스를 관리한다.

Notice that the major part of the swap time is transfer time. The total transfer time is directly proportional to the amount of memory swapped.

Swapping is constrained by other factors as well. If we want to swap a process, we must be sure that it is completely idle. 만약 I/O가 비동기로 I/O buffer를 위한 user memory에 접근 중이라면, 해당 프로세스는 swapped 될 수 없다.
If we were to swap out process P1 and swap in process P2, the I/O operation might then attempt to use memory that now belongs to process P2. There are two main solutions to this problem: never swap a process with pending I/O, or execute I/O operations only into operating-system buffers.
**double buffering** ??

Standard swappig은 현대 OS에서는 사용되지 않는다. 그건 swapping time도 많이 들고, execution time은 너무 작아서 reasonable한 memory-management solution이 될 수 없다. 대신 modifed version으로 사용된다.

* In one common variation, swapping is normally disabled but will start if the amount of free memory (unused memory available for the operating system or processes to use) falls below a threshold amount.

* Another variation involves swapping portions of processes—rather than entire processes—to decrease swap time.

Typically, these modified forms of swapping work in conjunction with virtual memory, which we cover in Chapter 9.

### 8.2.2 Swapping on Mobile Systems

모바일 환경에서는 typically swapping을 지원하지 않는다. 모바일 기기는 비교적 용량이 큰 hard disk가 아닌 flash memory를 쓰기 때문에, 모바일 OS designer들은 swapping을 피한다. 또한 flash memory는 쓰기 제한 있기 때문이기도 하다.
swapping을 쓰는 대신에, free memory가 일정 threshold 아래로 떨어지면, Apple의 iOS 같은 경우엔 application들이 자발적으로 메모리를 양도(reliquish)하도록 요청한다. Read-only data의 경우 메모리에서 삭제되고 수정된 data들은 남는다. 그러나 충분한 메모리를 만들어내지 못하면 어떤 application이든 Operating system에 의해 꺼질 수도 있다. Android도 iOS와 마찬가지로 swapping은 지원하지 않고, free memory가 부족하면 process를 죽일 수도 있다. 다만 **application state**를 flash memory가 써놓고 빠른 재시작이 가능하도록 한다.

## 8.3 Contiguous Memory Allocation

메인 메모리는 operating system과 user process들이 사용한 가능해야 한다. 그러므로 우리는 가능한한 효율적으로 main memory를 사용할 필요가 있다. 이번 장에선 예전 방법 중 하나인 contiguous(인접한) memory allocation을 설명하겠다.

The memory is usually divided into two partitions: one for the resident operating system and one for the user processes.

We usually want several user processes to reside in memory at the same time. We therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory. In contiguous memory allocation, each process is contained in a single section of memory that is contiguous to the section containing the next process.

각 프로세스는 다음 프로세스를 포함하는 섹션에 인접한 메모리의 단일 섹션에 포함됩니다 ?

### 8.3.1 Memory Protection

메모리 할당에 대해 더 이야기하기 전에 memory protection에 대해 먼저 짚어보자. 앞서 이야기한 2가지 idea를 복합사용함으로써, 프로세스가 소유하지 않은 메모리로의 접근을 막을 수 있다. relocation register와 limit register를 사용함으로써 목적을 달성할 수 있다. The relocation register contains the value of the smallest physical address; the limit register contains the range of logical addresses (for example, relocation = 100040 and limit = 74600).

Such code is sometimes called **transient** operating-system code; it comes and goes as needed. Thus, using this code changes the size of the operating system during program
execution.

### 8.3.2 Memory Allocation

할당하기 위해 분할하는 가장 쉬운 방법은 fixed-size로 partition을 만드는 것이다. 각각의 partition은 정확히 하나의 프로세스만을 가질 수 있다. 그러므로, multiprogramming degree는 partition의 개수에 의해 정해진다. **multiple partition method**이라 불리는 이 방법에서 partition이 비어 있으면 input queue로부터 선택된 process가 비어 있는 partition으로 로딩된다. 프로세스가 끝나면, 파티션은 다시 사용 가능해진다. IBM OS/360 operating system (called MFT)에서 사용되던 방법인데, 지금은 안 쓰인다. 

In the variable-partition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied. Initially, all memory is available for user processes and is considered one large block of available memory, a hole. Eventually, as you will see, memory contains a set of holes of various sizes.

Memory is allocated to processes until, finally, the memory requirements of the next process cannot be satisfied—that is, no available block of memory (or hole) is large enough to hold that process. The operating system can then wait until a large enough block is available, or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met.
If the new hole is adjacent to other holes, these adjacent holes are merged to form one larger hole. At this point, the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes.
This procedure is a particular instance of the general **dynamic storage allocation problem**, which concerns how to satisfy a request of size n from a list of free holes. There are many solutions to this problem. The first-fit, best-fit, and worst-fit strategies are the ones most commonly used to select a free hole from the set of available holes.

* **First fit.** Allocate the first hole that is big enough. Searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended.We can stop searching as soon as we find a free hole that is large enough.

* **Best fit.** Allocate the smallest hole that is big enough. We must search the entire list, unless the list is ordered by size. This strategy produces the smallest leftover hole.

* **Worst fit.** Allocate the largest hole. Again, we must search the entire list, unless it is sorted by size. This strategy produces the largest leftover hole, which may be more useful than the smaller leftover hole from a best-fit approach.

### 8.3.3 Fragmentation

Both the first-fit and best-fit strategies for memory allocation suffer from **external fragmentation**. process들이 작업을 끝내더라도 free memory space들이 조각들로 쪼개져 있어서, (not contiguous) 가용할 수 없게 된다.

Statistical analysis of first fit, for instance, reveals that, even with some optimization, given N allocated blocks, another 0.5 N blocks will be lost to fragmentation. That is, one-third of memory may be unusable! This property is known as the **50-percent rule**.

너무 작은 hole의 경우, hole의 크기보다 tracking하는 드는 overhead가 더 크다. 이런 문제가 생기지 않도록 physical memory를 fixed-sized block으로 쪼개서 unit별로 메모리를 할당해서 쓰기도 한다. 이렇게 되면 할당한 메모리에 비해 필요한 메모리는 작아서, **internal fragmentation**이 발생한다.

external fragmentation에 대한 솔루션 중에 하나로 **compaction**이 있다. The goal is to shuffle the memory contents so as to place all free memory together in one large block. Compaction is not always possible, however. If relocation is static and is done at assembly or load time, compaction cannot be done. It is possible only if relocation is dynamic and is done at execution time. If addresses are relocated dynamically, relocation requires only moving the program and data and then changing the base register to reflect the new base address. When compaction is possible, we must determine its cost. The simplest compaction algorithm is to move all processes toward one end of memory; all holes move in the other direction, producing one large hole of available memory. This scheme can be expensive.

Another possible solution to the external-fragmentation problem is to permit the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available. Two complementary techniques achieve this solution: segmentation (Section 8.4) and paging (Section 8.5). These techniques can also be combined.

Fragmentation is a general problem in computing that can occur wherever we must manage blocks of data. We discuss the topic further in the storage management chapters (Chapters 10 through and 12).

## 8.4 Segmentation

